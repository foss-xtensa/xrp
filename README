Xtensa Remote Processing.
========================

- Introduction
- Terminology
- Implementations
- Configuration
- Code structure
- Building

Xtensa Remote Processing (XRP) is a master-slave communication interface for
linux-based systems containing Xtensa processors. It allows linux userspace
tasks to send messages to the firmware running on Xtensa processors. Message
structure details are not defined by XRP, but it is assumed that the message
is a small structure that describes requested action accompanied by a vector
of buffers with data or storage for that action.

Terminology.
===========

XRP API uses the following terms:

- device: single Xtensa DSP core. Devices are numbered sequentially from 0
          to N - 1. There's an API call that opens device by its number.

- queue: a communication stream between an application and a device. Commands
         queued into one queue are processed in order and there's no ordering
         between commands in different queues.

- buffer: a descriptor for a contiguous piece of memory. A buffer must be
          mapped before its memory is accessible. A buffer must be unmapped
          before it may be passed to a device. A buffer may be allocated in
          the device specific memory or in the generic system memory. A buffer
          in the device specific memory is the most effective way to pass
          bulk data to/from the Xtensa DSP, it is physically contiguous and
          data transfer does not require copying. Buffer in the generic system
          memory may be physically noncontiguous and tranferring data may
          require copying to/from the intermediate bounce buffer.

- buffer group: a vector of buffers and associated allowed access flags.
                Buffers are numbered sequentially from 0 to N - 1. A buffer
                may be added to the end of a group. A buffer descriptor may be
                acquired from the buffer group by index. A buffer may be added
                to a buffer group with the access flags allowing reading,
                writing or both. These access flags limit the ways in which
                the buffer descriptor acquired from the buffer group may be
                mapped.

- message: a unit of communication between the XRP API users. A message may be
           sent by the master to a queue synchronously or asynchronously. It
           is processed synchronously by the Xtensa DSP and the reply is
           delivered back to master. If a message was sent synchronously the
           sender is unblocked at that point. If a message was sent
           asynchronously the sender gets an event object that can be used to
           wait for the message processing completion.
           A message takes a small read-only region of memory with a message
           description data, a small write-only region of memory where the
           result description data is written and a buffer group with buffers
           for the bulk data being processed.

Devices, queues, buffers, buffer groups and events are reference-counted. For
each object type there's a couple of functions that increment and decrement
object's reference counter. An object is freed when its reference counter
drops to zero.

XRP implementations.
===================

There are two implementations of the XRP interface: native and fast simulation.

Native implementation runs on a system where CPU cores running host linux code
and Xtensa DSP cores are connected to shared memory. It may be a hardware or
a simulated system. It consists of the following three parts: native linux
implementation of the XRP interface, linux kernel driver and DSP implementation
of the XRP interface.
- native linux XRP implementation is a library code that runs in linux
  userspace processes on linux CPU cores. It communicates with devices managed
  by the linux kernel driver through the file-based interface exposed by the
  driver;
- linux kernel driver does the following:
  - manages Xtensa DSP cores: loads firmware into them, configures shared
    memory/used IRQs, performs initial synchronization and manages request
    queue;
  - manages dedicated physical memory of the Xtensa DSP cores;
  - maps and unmaps userspace buffers associated with requests making them
    physically contiguous.
- Xtensa DSP side XRP implementation is a library code that runs as a part
  of firmware on Xtensa DSP implementing DSP subset of the XRP API. It
  receives requests, invokes handler and sends replies back to host.

Fast simulation runs linux-side code on a linux-based system and communicates
with unmodiified Xtensa DSP code running in a simulator on the same linux-based
system. It consists of the following two parts: fast simulation implementation
of the XRP interface and the DSP implementation of the XRP interface.
- fast simulation XRP implementation is a library code that runs in linux
  userspace process and communicates through the shared memory with the Xtensa
  DSP side XRP code running in simulator;
- Xtensa DSP side XRP implementation is the same both in native and in fast
  simulation cases.

Kernel driver requirements.
==========================

The linux kernel driver is well tested with linux kernel version 4.8 and is
known to work with linux kernel version 4.3 on xtensa and 32-bit ARM linux.
The driver makes the following assumptions:
- there exists reset and runstall memory-mapped registers for each DSP core,
  DSP doesn't need any additional setup to run (i.e. no clock setup, no power
  management, default reset vector);
- there may exist memory-mapped register for sending IRQ to DSP core; if it
  exists then in addition to always available polling mode the DSP firmware
  may run in IRQ mode;
- there may exist memory-mapped register for sending IRQ from DSP to the host;
  if it exists then in addition to always available polling mode the linux
  driver may run in IRQ mode.
- DSP DRAM and IRAM are writable from the linux host. Host may access DRAM
  and IRAM at physical addresses different from their physical addresses used
  by the DSP (i.e. multiple DSPs may have identical configurations, and thus
  IRAM and DRAM at the same addresses, but the host need to see each IRAM
  and DRAM at physical addresses that don't overlap).
- the driver provides functions to manage contiguous memory allocation for the
  DSP. These functions manage physical memory pool configured for the DSP.
  The DSP must have access to this physical memory at the same physical
  address as the host.

Configuration.
=============

Number and configuration of Xtensa DSPs, details of MMIO registers that
control them, location and amount of the shared memory all may change. The
following pieces of the code capture the details of the configuration and need
to be adjusted whenever it is changed:

Native mode:
- the device tree. Linux kernel need to have a device tree node for each
  Xtensa DSP managed by the XRP linux kernel driver. Please see
  xrp-kernel/cdns,xrp.txt for the device tree binding information.
- DSP firmware bits of the XRP don't have any hardcoded addresses, but the
  firmware image depends on the linker map.

Fast simulation mode:
- the device tree. Although there's no linux kernel and kernel driver involved,
  the configuration is maintained in the same format as in native mode. But in
  addition to a device node for each managed Xtensa DSP there must be a device
  node for all segments of memory shared with Xtensa simulators. Please see
  xrp-linux-sim/cdns,sim-shmem.txt for the device tree binding information.
- DSP firmware bits of the XRP built for fast simulation have the base address
  of the communication area in the shared memory built into it. See how
  XRP_DSP_COMM_BASE_MAGIC is used.

Code structure.
==============

+-dts/              -- device tree compiler/flat device tree library. May be
|                      used with xrp-linux-sim when the host version of libfdt
|                      is missing or too old.
+-xrp-dsp/          -- Xtensa DSP side XRP implementation.
+-xrp-kernel/       -- linux kernel driver for the native XRP implementation.
| `-test/           -- standalone tests for the driver.
+-xrp-linux-native/ -- linux implementation of the XRP interface for the native
|                      mode.
+-xrp-linux-sim/    -- linux implementation of the XRP interface for the fast
|                      simulation mode.
`-xrp_api.h         -- XRP API definition.

Building.
========

- kernel driver is built using typical kernel module build sequence. Provide
  the following environment variables/make parameters:
  - ARCH: target linux architecture;
  - KSRC: points to the configured kernel tree;
  - CROSS_COMPILE: path and prefix name of the cross compiler.
  Makefile supports targets "modules", "clean" and all other standard
  kernel targets. Example building for xtensa linux:

  $ ARCH=xtensa CROSS_COMPILE=xtensa-dc233c-elf- \
    KSRC=`pwd`/../kernel/xrp-sim-dc233c make -C xrp-kernel modules

  The result is kernel object xrp-kernel/xrp.ko loadable into the kernel it was
  built against.

- Xtensa DSP code needs an xcc-based toolchain. Provide the following
  environment variables to the make:
  - DSP_CORE: name of the Xtensa DSP core to build for. It must be the name of
    the build installed for the used xcc-based toolchain;
  - LSP: path to the LSP to be used.
  Makefile supports targets "default" and "clean". Example:

  $ make -C xrp-dsp DSP_CORE=visionp6cnn_ao \
    LSP=MW-MP/P6_1/xtensa-elf/lib/sim-stack-local

  The result is xrp-dsp/xrp-dsp. In order to be loaded as a firmware it needs
  to be put to the linux file system under /lib/modules/firmware and its name
  need to be specified in the "firmware" parameter inside xrp device node in
  the device tree.

- linux side code needs a toolchain for the target linux. Makefile supports
  targets "default" and "clean". Examples:

  $ make -C xrp-linux-native
  $ make -C xrp-linux-sim
